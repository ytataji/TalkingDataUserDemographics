{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "IPython.tab_as_tab_everywhere = function(use_tabs) {\n",
       "    if (use_tabs === undefined) {\n",
       "        use_tabs = true; \n",
       "    }\n",
       "\n",
       "    // apply setting to all current CodeMirror instances\n",
       "    IPython.notebook.get_cells().map(\n",
       "        function(c) {  return c.code_mirror.options.indentWithTabs=use_tabs;  }\n",
       "    );\n",
       "    // make sure new CodeMirror instances created in the future also use this setting\n",
       "    CodeMirror.defaults.indentWithTabs=use_tabs;\n",
       "\n",
       "    };\n",
       "\n",
       "IPython.tab_as_tab_everywhere()\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "\n",
    "IPython.tab_as_tab_everywhere = function(use_tabs) {\n",
    "    if (use_tabs === undefined) {\n",
    "        use_tabs = true; \n",
    "    }\n",
    "\n",
    "    // apply setting to all current CodeMirror instances\n",
    "    IPython.notebook.get_cells().map(\n",
    "        function(c) {  return c.code_mirror.options.indentWithTabs=use_tabs;  }\n",
    "    );\n",
    "    // make sure new CodeMirror instances created in the future also use this setting\n",
    "    CodeMirror.defaults.indentWithTabs=use_tabs;\n",
    "\n",
    "    };\n",
    "\n",
    "IPython.tab_as_tab_everywhere()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_fun_1(X):\n",
    "\t\"\"\"This Function will take test input and return the predicted values related to different classes\"\"\"\n",
    "\timport pandas as pd\n",
    "\timport numpy as np\n",
    "\timport matplotlib.pyplot as plt\n",
    "\t%matplotlib inline\n",
    "\timport seaborn as sns\n",
    "\timport os\n",
    "\timport re\n",
    "\timport time\n",
    "\tfrom sklearn.metrics import confusion_matrix\n",
    "\tfrom sklearn.metrics.classification import log_loss\n",
    "\tfrom sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\tfrom sklearn.feature_extraction.text import CountVectorizer\n",
    "\tfrom collections import Counter\n",
    "\tfrom scipy.sparse import hstack\n",
    "\tfrom scipy.sparse import csr_matrix\n",
    "\timport warnings\n",
    "\twarnings.filterwarnings(\"ignore\")\n",
    "\tfrom datetime import datetime\n",
    "\tfrom sklearn.model_selection import train_test_split\n",
    "\tfrom sklearn.linear_model import LogisticRegression\n",
    "\tfrom sklearn.ensemble import RandomForestClassifier\n",
    "\tfrom sklearn.calibration import CalibratedClassifierCV\n",
    "\tfrom xgboost import XGBClassifier\n",
    "\tfrom sklearn.model_selection import RandomizedSearchCV\n",
    "\tfrom sklearn.preprocessing import LabelEncoder\n",
    "\tfrom sklearn.preprocessing import StandardScaler\n",
    "\tfrom keras.models import Sequential\n",
    "\tfrom keras.layers import Dense, Dropout, Activation, BatchNormalization,Input,PReLU\n",
    "\tfrom keras.utils import np_utils\n",
    "\tfrom keras.optimizers import Adam\n",
    "\tfrom keras.models import Model\n",
    "\tfrom keras.optimizers import Adagrad\n",
    "\tfrom sklearn.externals import joblib\n",
    "\tstart=datetime.now()\n",
    "\ttrain_data=pd.read_csv('Train_Data_with_events_details.csv')\n",
    "\ttest_data=pd.read_csv('Test_Data_with_evetns_details.csv')\n",
    "\ttrain_data.drop(['Unnamed: 0'],axis=1,inplace=True)#removing the first column which does nt have any column name...\n",
    "\ttest_data.drop(['Unnamed: 0'],axis=1,inplace=True)#removing the first column which does not have any column name....\n",
    "\tevents_train_data=train_data.loc[train_data['has_events']==True]#To get data with events\n",
    "\tevents_test_data=test_data.loc[test_data['has_events']==True]\n",
    "\tnoevents_train_data=train_data.loc[train_data['has_events']==False]#to get data without events\n",
    "\tnoevents_test_data=test_data.loc[test_data['has_events']==False]\n",
    "\tevents_train_data.drop(['has_events'],axis=1,inplace=True)# we are keeping only device id by dropping has_events column\n",
    "\tevents_test_data.drop(['has_events'],axis=1,inplace=True)\n",
    "\tevents = pd.read_csv('events.csv',  parse_dates=['timestamp'], index_col='event_id')\n",
    "\tphone_data=pd.read_csv('phone_brand_device_model.csv')\n",
    "\t# we are droping Duplicate Devices and set Device_id as index like we did for Data while importing\n",
    "\tphone_data = phone_data.drop_duplicates('device_id',keep='first').set_index('device_id')\n",
    "\t# we take event_id, app_id and is_active.we dont take is_installed feature as it is not much useful\n",
    "\tapp_events = pd.read_csv('app_events.csv', usecols=['event_id','app_id','is_active'], dtype={'is_active':bool})#we are treating is_active data type as boolean\n",
    "\tapp_labels = pd.read_csv('app_labels.csv')\n",
    "\tlabel_categories = pd.read_csv('label_categories.csv')\n",
    "\ttrain_data=train_data.set_index('device_id')\n",
    "\ttest_data=test_data.set_index('device_id')\n",
    "\tevents_train_data=events_train_data.set_index('device_id')\n",
    "\tevents_test_data=events_test_data.set_index('device_id')\n",
    "\tnoevents_train_data=noevents_train_data.set_index('device_id')\n",
    "\tnoevents_test_data=noevents_test_data.set_index('device_id')\n",
    "\t#we will use numpy.arange function to create row numbers \n",
    "\ttrain_data['trainrow']=np.arange(train_data.shape[0])#all devices\n",
    "\tevents_train_data['trainrow']=np.arange(events_train_data.shape[0])#devices with events\n",
    "\ttest_data['testrow']=np.arange(test_data.shape[0])#all devices\n",
    "\tevents_test_data['testrow']=np.arange(events_test_data.shape[0])#devices with events\n",
    "\tnoevents_train_data['trainrow']=np.arange(noevents_train_data.shape[0])#devices without events\n",
    "\tnoevents_test_data['testrow']=np.arange(noevents_test_data.shape[0])#devices without events\n",
    "\t#Converting Categorical Brands in phone data to Integers from 0 to number of unique brands -1\n",
    "\tbrand_encoder = LabelEncoder().fit(phone_data['phone_brand'])\n",
    "\tphone_data['brand'] = brand_encoder.transform(phone_data['phone_brand'])\n",
    "\tnbrands=len(brand_encoder.classes_)# number of unique apps it will be used in creating One-Hot Encoding of Brands\n",
    "\t#Concatinating Phone Brand and Model \n",
    "\tconcat_model = phone_data['phone_brand'].str.cat(phone_data['device_model'])\n",
    "\t#Converting Categorical Maodel in phone data  to Integers from 0 to number of unique models -1\n",
    "\tmodel_encoder=LabelEncoder().fit(concat_model)\n",
    "\tphone_data['model_brand']=model_encoder.transform(concat_model)\n",
    "\tnmodels=len(model_encoder.classes_)# number of unique apps it will be used in creating One-Hot Encoding of Models\n",
    "\tmodel_encode=LabelEncoder().fit(phone_data['device_model'])\n",
    "\tphone_data['model']=model_encode.transform(phone_data['device_model'])\n",
    "\tnum_models=len(model_encoder.classes_)\n",
    "\t#Both the dataframes have device_id as indices so phone_brand and model corresponding to a device_id get copied to appropriate\n",
    "#columns\n",
    "\ttrain_data['phone_brand']=phone_data['brand']#adding encoded brand data to train data\n",
    "\ttest_data['phone_brand']=phone_data['brand']#adding encoded brand data to test data\n",
    "\ttrain_data['phone_model']=phone_data['model']#adding encoded model data to train data\n",
    "\ttest_data['phone_model']=phone_data['model']#adding encoded model data to test data\n",
    "\tevents_train_data['phone_brand']=phone_data['brand']#adding encoded brand data to train data with events\n",
    "\tevents_test_data['phone_brand']=phone_data['brand']#adding encoded brand data to test data with events\n",
    "\tevents_train_data['phone_model']=phone_data['model_brand']#addding encoded model+brand data to train data with events\n",
    "\tevents_test_data['phone_model']=phone_data['model_brand']#adding encoded model+brand data to test data with events\n",
    "\t#we dont add encoded model+brand data to nonevents data\n",
    "\tnoevents_train_data['phone_brand']=phone_data['brand']#adding encoded brand data to train data without events\n",
    "\tnoevents_test_data['phone_brand']=phone_data['brand']#adding encoded brand data to test data without events\n",
    "\tnoevents_train_data['phone_model']=phone_data['model']#adding encoded model data to train data without events\n",
    "\tnoevents_test_data['phone_model']=phone_data['model']#adding encoded model data to test data without events \n",
    "\tapp_encoder = LabelEncoder().fit(app_events['app_id'])\n",
    "\tapp_events['app'] = app_encoder.transform(app_events['app_id'])\n",
    "\tnapps = len(app_encoder.classes_)# number of unique apps it will be used in creating One-Hot Encoding of Apps \n",
    "\tdeviceapps = (app_events.merge(events[['device_id']], how='left',left_on='event_id',right_index=True)\n",
    "                       .groupby(['device_id','app'])['app'].agg(['size'])\n",
    "                       .merge(events_train_data[['trainrow']], how='left', left_index=True, right_index=True)#merges by index which is device_id from train data with events\n",
    "                       .merge(events_test_data[['testrow']], how='left', left_index=True, right_index=True)#merges by index which is device_id from test data with events\n",
    "                       .reset_index())\n",
    "\tapp_labels = app_labels.loc[app_labels['app_id'].isin(app_events['app_id'].unique())]#we are taking app_id's existed both in app_events and app_labels\n",
    "\tapp_labels['app'] = app_encoder.transform(app_labels['app_id'])#encoding common app_id's from above line\n",
    "\tlabelencoder = LabelEncoder().fit(app_labels['label_id'])#now encoding app_labels\n",
    "\tapp_labels['label'] = labelencoder.transform(app_labels['label_id'])\n",
    "\tnlabels = len(labelencoder.classes_)\n",
    "\tdevicelabels = (deviceapps[['device_id','app']]\n",
    "                .merge(app_labels[['app','label']])\n",
    "                .groupby(['device_id','label'])['app'].agg(['size'])\n",
    "                .merge(events_train_data[['trainrow']], how='left', left_index=True, right_index=True)#merges by index which is device_id from train data with events\n",
    "                .merge(events_test_data[['testrow']], how='left', left_index=True, right_index=True)#merges by index which is device_id from test data with events\n",
    "                .reset_index())#resetting index to default indices\n",
    "\t#Extracting Hours from Events Time Stamp by using map function \n",
    "\tevents['hour'] = events['timestamp'].map(lambda x:pd.to_datetime(x).hour)\n",
    "\tevents['hourbin'] = [1 if ((x>=1)&(x<=6)) else 2 if ((x>=7)&(x<=12)) else 3 if ((x>=13)&(x<=18)) else 4 for x in events['hour']]\n",
    "\thourevents = events.groupby(\"device_id\")[\"hour\"].apply(lambda x: \" \".join('0'+str(s) for s in x))\n",
    "\thourbinevents = events.groupby(\"device_id\")[\"hourbin\"].apply(lambda x: \" \".join('0'+str(s) for s in x))\n",
    "\t#Mapping The Values the values to train and test dataframes\n",
    "\tevents_train_data['event_hours']=events_train_data.index.map(hourevents)#mapping hourevents to train data with events based on index deviceid\n",
    "\tevents_test_data['event_hours']=events_test_data.index.map(hourevents)#mapping hourevents to test data with events based on index deviceid\n",
    "\tevents_train_data['event_hours_bins']=events_train_data.index.map(hourbinevents)#mapping houreventsbin to train data with events based on index deviceid\n",
    "\tevents_test_data['event_hours_bins']=events_test_data.index.map(hourbinevents)#mapping houreventsbins to test data with events based on index deviceid\n",
    "\t#Extracting Days of the week\n",
    "\tdays_of_week=events['timestamp'].dt.day_name()\n",
    "\tevents['day']=days_of_week.map({'Sunday':0,'Monday':1,'Tuesday':2,'Wednesday':3,'Thursday':4,'Friday':5,'Saturday':6})#mapping day to numeric digits\n",
    "\tdaysevents = events.groupby(\"device_id\")[\"day\"].apply(lambda x: \" \".join('0'+str(s) for s in x))#concatening '0' to all single nubmerals\n",
    "\tevents_train_data['event_day']=events_train_data.index.map(daysevents)#mapping days to train data with events\n",
    "\tevents_test_data['event_day']=events_test_data.index.map(daysevents)#mapping days to test data with events\n",
    "\t#Grouping by device_id and taking the median of latitude\n",
    "\tlat_events = events.groupby(\"device_id\")[\"latitude\"].apply(lambda x: np.median([float(s) for s in x]))\n",
    "\t#Grouping by device_id and taking the median of longitude\n",
    "\tlong_events = events.groupby(\"device_id\")[\"longitude\"].apply(lambda x: np.median([float(s) for s in x]))\n",
    "\tevents_train_data['event_med_lat']=events_train_data.index.map(lat_events)#mapping lattitude_medians to events_train_data\n",
    "\tevents_test_data['event_med_lat']=events_test_data.index.map(lat_events)#mapping lattitude_medians to events_test_data\n",
    "\tevents_train_data['event_med_long']=events_train_data.index.map(long_events)#mapping longitude_medians to events_train_data\n",
    "\tevents_test_data['event_med_long']=events_test_data.index.map(long_events)#mapping longitude_medians to events_test_data\n",
    "\tappsactive = app_events.groupby(\"event_id\")[\"is_active\"].apply(lambda x: \" \".join(str(s) for s in x))#converting boolean to strings\n",
    "\t#Mapping apps is_active to device_id\n",
    "\tevents[\"apps_active\"] = events.index.map(appsactive)#mapping converted strings to events\n",
    "\tevents_apps_active_map = events.groupby(\"device_id\")[\"apps_active\"].apply(lambda x: \" \".join(str(s) for s in x if str(s)!='nan'))#mapping only non-nan(status of is_active) acitve apps\n",
    "\t#Mapping The Values the values to train and test dataframes\n",
    "\tevents_train_data['apps_active']=events_train_data.index.map(events_apps_active_map)#mapping non-nan statused apps to train data with events\n",
    "\tevents_test_data['apps_active']=events_test_data.index.map(events_apps_active_map)#mapping non-nan statused apps to test data with events\n",
    "\ttrain_data_store=events_train_data.reset_index()\n",
    "\ttest_data_store=events_test_data.reset_index()\n",
    "\ttrain_data_noevents=noevents_train_data.reset_index()\n",
    "\ttest_data_noevents=noevents_test_data.reset_index()\n",
    "\t#Storing the Processed Events Train Data\n",
    "\ttrain_data_store.to_csv('Processed_events_train_data.csv')\n",
    "\t#Storing the Processed Events Test\n",
    "\ttest_data_store.to_csv('Processed_events_test_data.csv')\n",
    "\t#Storing the Processed No Events Train Data\n",
    "\ttrain_data_noevents.to_csv('Processed_no_events_train_data.csv')\n",
    "\t#Storing the Processed No Events Test\n",
    "\ttest_data_noevents.to_csv('Processed_no_events_test_data.csv')\n",
    "\t#https://stackoverflow.com/questions/8955448/save-load-scipy-sparse-csr-matrix-in-portable-data-format\n",
    "\tdef save_sparse(filename, xmtr):\n",
    "\t\t\"\"\"\n",
    "\t\tTakes the Sparse Matrix, file name as input and saves the Matrix as npz file\n",
    "\t\t\"\"\"\n",
    "\t\tnp.savez(filename,data = xmtr.data ,indices= xmtr.indices,\n",
    "             indptr =xmtr.indptr, shape=xmtr.shape )\n",
    "\t#https://stackoverflow.com/questions/8955448/save-load-scipy-sparse-csr-matrix-in-portable-data-format\n",
    "\tdef load_sparse(filename):\n",
    "\t\t\"\"\"\n",
    "\t\tTakes the filename as input and loads the matrix stored as .npz file to a csr matrix and returns the matrix\n",
    "\t\t\"\"\"\n",
    "\t\ttmp = np.load(filename)\n",
    "\t\treturn csr_matrix((tmp['data'], tmp['indices'], tmp['indptr']), shape= tmp['shape'])\n",
    "\tXtr_brand = csr_matrix((np.ones(train_data.shape[0]), # Number of Rows/Devices\n",
    "                       (train_data.trainrow, train_data.phone_brand)))# columns as brands \n",
    "\tXte_brand = csr_matrix((np.ones(test_data.shape[0]), # Number of Rows/Devices\n",
    "                       (test_data.testrow, test_data.phone_brand)))\n",
    "\tXtr_model = csr_matrix((np.ones(train_data.shape[0]), # Number of Rows/Devices\n",
    "                       (train_data.trainrow, train_data.phone_model)))\n",
    "\tXte_model = csr_matrix((np.ones(test_data.shape[0]), # Number of Rows/Devices\n",
    "                       (test_data.testrow, test_data.phone_model)))\n",
    "\tX_train_one_hot=hstack((Xtr_brand,Xtr_model),format='csr')\n",
    "\tX_test_one_hot=hstack((Xte_brand,Xte_model),format='csr')\n",
    "\tsave_sparse('Train_One_hot_brand_model_matrix',X_train_one_hot)\n",
    "\tsave_sparse('Test_One_hot_brand_model_matrix',X_test_one_hot)\n",
    "\t#https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html\n",
    "\tXtr_noevents_brand = csr_matrix((np.ones(noevents_train_data.shape[0]), # Number of Rows/Devices\n",
    "                       (noevents_train_data.trainrow, noevents_train_data.phone_brand)))\n",
    "\tXte_noevents_brand = csr_matrix((np.ones(noevents_test_data.shape[0]), # Number of Rows/Devices\n",
    "                       (noevents_test_data.testrow, noevents_test_data.phone_brand)))\n",
    "\tXtr_noevents_model = csr_matrix((np.ones(noevents_train_data.shape[0]), # Number of Rows/Devices\n",
    "                       (noevents_train_data.trainrow, noevents_train_data.phone_model)))\n",
    "\tXte_noevents_model = csr_matrix((np.ones(noevents_test_data.shape[0]), # Number of Rows/Devices\n",
    "                       (noevents_test_data.testrow, noevents_test_data.phone_model)))\n",
    "\tX_train_noevents_one_hot=hstack((Xtr_noevents_brand,Xtr_noevents_model),format='csr')\n",
    "\tX_test_noevents_one_hot=hstack((Xte_noevents_brand,Xte_noevents_model),format='csr')\n",
    "\t#Saving One-hot encoded Matrices\n",
    "\tsave_sparse('Train_Noevents_One_hot_brand_model_matrix',X_train_noevents_one_hot)\n",
    "\tsave_sparse('Test_Noevents_One_hot_brand_model_matrix',X_test_noevents_one_hot)\n",
    "\t#https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html\n",
    "\tXtr_events_brand = csr_matrix((np.ones(events_train_data.shape[0]), # Number of Rows/Devices\n",
    "                       (events_train_data.trainrow, events_train_data.phone_brand)), \n",
    "                              shape=(events_train_data.shape[0],nbrands))\n",
    "\tXte_events_brand = csr_matrix((np.ones(events_test_data.shape[0]), # Number of Rows/Devices\n",
    "                       (events_test_data.testrow, events_test_data.phone_brand)),\n",
    "                             shape=(events_test_data.shape[0],nbrands))\n",
    "\t#https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html\n",
    "\tXtr_events_model = csr_matrix((np.ones(events_train_data.shape[0]), # Number of Rows/Devices\n",
    "                       (events_train_data.trainrow, events_train_data.phone_model)),\n",
    "                         shape=(events_train_data.shape[0],nmodels))\n",
    "\tXte_events_model = csr_matrix((np.ones(events_test_data.shape[0]), # Number of Rows/Devices\n",
    "                       (events_test_data.testrow, events_test_data.phone_model)),\n",
    "                           shape=(events_test_data.shape[0],nmodels))\n",
    "\t#Since the Deviceapps has both train and test columns merged to create Train Apps One-Hot we will Drop all Nan of Train Row\n",
    "\t#Once we remove Nan in Train Rows we will get the Apps in Train Data and we create CSR Matrix for those rows\n",
    "\td = deviceapps.dropna(subset=['trainrow'])#based on trainrow we drop\n",
    "\tXtr_events_app = csr_matrix((np.ones(d.shape[0]), (d.trainrow, d.app)), \n",
    "                      shape=(events_train_data.shape[0],napps))\n",
    "\t#Since the Deviceapps has both train and test columns merged to create Test Apps One-Hot we will Drop all Nan of Test Row\n",
    "\t#Once we remove Nan in Test Rows we will get the Apps in Test Data and we create CSR Matrix for those rows\n",
    "\td = deviceapps.dropna(subset=['testrow'])#based on test row we drop\n",
    "\tXte_events_app = csr_matrix((np.ones(d.shape[0]), (d.testrow, d.app)), \n",
    "                      shape=(events_test_data.shape[0],napps))\n",
    "\t#Since the Devicelabels has both train and test columns merged to create Train Labels One-Hot we will Drop all Nan of Train Row\n",
    "\t#Once we remove Nan in Train Rows we will get the Labels in Train Data and we create CSR Matrix for those rows\n",
    "\td = devicelabels.dropna(subset=['trainrow'])##based on trainrow we drop\n",
    "\tXtr_events_label = csr_matrix((np.ones(d.shape[0]), (d.trainrow, d.label)), \n",
    "                      shape=(events_train_data.shape[0],nlabels))\n",
    "\t#Since the Devicelabels has both train and test columns merged to create Test Labels One-Hot we will Drop all Nan of Test Row\n",
    "\t#Once we remove Nan in Test Rows we will get the Labels in Test Data and we create CSR Matrix for those rows\n",
    "\td = devicelabels.dropna(subset=['testrow'])#based on test row  we drop\n",
    "\tXte_events_label = csr_matrix((np.ones(d.shape[0]), (d.testrow, d.label)), \n",
    "                      shape=(events_test_data.shape[0],nlabels))\n",
    "\tvectorizer_EH=TfidfVectorizer()\n",
    "\tvectorizer_EH.fit(events_train_data['event_hours'].values)\n",
    "\n",
    "\tX_tr_event_hours_one_hot = vectorizer_EH.transform(events_train_data['event_hours'].values)\n",
    "\tX_te_event_hours_one_hot = vectorizer_EH.transform(events_test_data['event_hours'].values)\n",
    "\tvectorizer_EHB=CountVectorizer(binary=True)\n",
    "\tvectorizer_EHB.fit(events_train_data['event_hours_bins'].values)\n",
    "\tX_tr_event_hours_bins_one_hot = vectorizer_EHB.transform(events_train_data['event_hours_bins'].values)\n",
    "\tX_te_event_hours_bins_one_hot = vectorizer_EHB.transform(events_test_data['event_hours_bins'].values)\n",
    "\tvectorizer_ED=TfidfVectorizer()\n",
    "\tvectorizer_ED.fit(events_train_data['event_day'].values)\n",
    "\tX_tr_event_day_one_hot = vectorizer_ED.transform(events_train_data['event_day'].values)\n",
    "\tX_te_event_day_one_hot = vectorizer_ED.transform(events_test_data['event_day'].values)\n",
    "\tscaler_lat=StandardScaler()\n",
    "\tscaler_lat.fit(events_train_data['event_med_lat'].values.reshape(-1,1))\n",
    "\n",
    "\tX_tr_event_med_lat_scaled = scaler_lat.transform(events_train_data['event_med_lat'].values.reshape(-1,1))\n",
    "\tX_te_event_med_lat_scaled = scaler_lat.transform(events_test_data['event_med_lat'].values.reshape(-1,1))\n",
    "\tscaler_long=StandardScaler()\n",
    "\tscaler_long.fit(events_train_data['event_med_long'].values.reshape(-1,1))\n",
    "\n",
    "\tX_tr_event_med_long_scaled = scaler_long.transform(events_train_data['event_med_long'].values.reshape(-1,1))\n",
    "\tX_te_event_med_long_scaled = scaler_long.transform(events_test_data['event_med_long'].values.reshape(-1,1))\n",
    "\tvectorizer_AA=TfidfVectorizer()\n",
    "\tvectorizer_AA.fit(events_train_data['apps_active'].values)\n",
    "\n",
    "\tX_tr_apps_active_one_hot = vectorizer_AA.transform(events_train_data['apps_active'].values)\n",
    "\tX_te_apps_active_one_hot = vectorizer_AA.transform(events_test_data['apps_active'].values)\n",
    "\t#Converting to Sparse Matrices\n",
    "\tX_tr_event_hours_one_hot=X_tr_event_hours_one_hot.tocsr()\n",
    "\tX_te_event_hours_one_hot=X_te_event_hours_one_hot.tocsr()\n",
    "\n",
    "\tX_tr_event_hours_bins_one_hot=X_tr_event_hours_bins_one_hot.tocsr()\n",
    "\tX_te_event_hours_bins_one_hot=X_te_event_hours_bins_one_hot.tocsr()\n",
    "\n",
    "\tX_tr_event_day_one_hot=X_tr_event_day_one_hot.tocsr()\n",
    "\tX_te_event_day_one_hot=X_te_event_day_one_hot.tocsr()\n",
    "\n",
    "\tX_tr_apps_active_one_hot=X_tr_apps_active_one_hot.tocsr()\n",
    "\tX_te_apps_active_one_hot=X_te_apps_active_one_hot.tocsr()\n",
    "\n",
    "\tX_train_events_one_hot_1=hstack((Xtr_events_brand,Xtr_events_model,Xtr_events_label,X_tr_event_hours_one_hot,X_tr_event_hours_bins_one_hot,X_tr_event_day_one_hot,X_tr_event_med_lat_scaled,X_tr_event_med_long_scaled,Xtr_events_app,X_tr_apps_active_one_hot),format='csr')\n",
    "\tX_test_events_one_hot_1=hstack((Xte_events_brand,Xte_events_model,Xte_events_label,X_te_event_hours_one_hot,X_te_event_hours_bins_one_hot,X_te_event_day_one_hot,X_te_event_med_lat_scaled,X_te_event_med_long_scaled,Xte_events_app,X_te_apps_active_one_hot),format='csr')\n",
    "\t#Saving One-hot encoded Matrices\n",
    "\tsave_sparse('Train_Events_One_hot_matrix_1',X_train_events_one_hot_1)\n",
    "\tsave_sparse('Test_Events_One_hot_matrix_1',X_test_events_one_hot_1)\n",
    "\ty_data=train_data['Class'].values\n",
    "\ttrain_1, cv_1, y_train_1, y_cv_1 = train_test_split(X_train_one_hot, y_data,stratify=y_data,test_size=0.15,random_state=18)\n",
    "\ttest_1=X_test_noevents_one_hot#because we want to test on no events data\n",
    "\tlr_sig_clf=joblib.load('calibrated_LR_no_events.sav')\n",
    "\tlr_no_events_test_prediction=lr_sig_clf.predict_proba(test_1)\n",
    "\t#Loading All 5 Saved Model_1_1 Neural Network  Models\n",
    "\tdef model_1_1(input_shape):\n",
    "\t\tmodel = Sequential()\n",
    "\t\tmodel.add(Dense(256, input_dim=input_shape))\n",
    "\t\tmodel.add(PReLU())\n",
    "\t\tmodel.add(BatchNormalization())\n",
    "\t\tmodel.add(Dropout(0.5))\n",
    "\t\tmodel.add(Dense(64))\n",
    "\t\tmodel.add(PReLU())\n",
    "\t\tmodel.add(BatchNormalization())\n",
    "\t\tmodel.add(Dropout(0.5))\n",
    "\t\tmodel.add(Dense(12))\n",
    "\t\tmodel.add(Activation('softmax'))\n",
    "\t\tmodel.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\t\treturn model\n",
    "\tdef create_model_1_2(input_dim,output_dim, learRate=0.0025):\n",
    "\t\tmodel = Sequential()\n",
    "\t\tmodel.add(Dense(500, input_shape=(input_dim,), init='uniform'))\n",
    "\t\tmodel.add(PReLU(init='zero'))\n",
    "\t\tmodel.add(Dropout(0.82))\n",
    "\t\tmodel.add(Dense(output_dim, init='uniform'))\n",
    "\t\tmodel.add(Activation('softmax'))\n",
    "\t\topt = Adagrad(lr=learRate, epsilon=1e-08)\n",
    "\t\tmodel.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "\t\treturn model\n",
    "\tdef model_2_1(input_dim,output_dim):\n",
    "\t\tmodel = Sequential()\n",
    "\t\tmodel.add(Dropout(0.15, input_shape=(input_dim,)))\n",
    "\t\tmodel.add(Dense(240, init='uniform'))\n",
    "\t\tmodel.add(PReLU(init='zero'))\n",
    "\t\tmodel.add(Dropout(0.8))\n",
    "\t\tmodel.add(Dense(240, init='uniform'))\n",
    "\t\tmodel.add(PReLU(init='zero', weights=None))\n",
    "\t\tmodel.add(Dropout(0.35))\n",
    "\t\tmodel.add(Dense(260, init='uniform'))\n",
    "\t\tmodel.add(PReLU(init='zero', weights=None))\n",
    "\t\tmodel.add(Dropout(0.40))\n",
    "\t\tmodel.add(Dense(output_dim, init='uniform'))\n",
    "\t\tmodel.add(Activation('softmax'))\n",
    "\t\topt = Adagrad(lr=0.008, epsilon=1e-08)\n",
    "\t\tmodel.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "\t\treturn model\n",
    "\tdef model_2_2(input_dim,output_dim):\n",
    "\t\tmodel = Sequential()\n",
    "\t\tmodel.add(Dropout(0.4, input_shape=(input_dim,)))\n",
    "\t\tmodel.add(Dense(75))\n",
    "\t\tmodel.add(PReLU())\n",
    "\t\tmodel.add(Dropout(0.30))\n",
    "\t\tmodel.add(Dense(50, init='normal', activation='tanh'))\n",
    "\t\tmodel.add(PReLU())\n",
    "\t\tmodel.add(Dropout(0.20))\n",
    "\t\tmodel.add(Dense(output_dim, init='normal', activation='softmax'))\n",
    "\t\tmodel.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "\t\treturn model\n",
    "\tmodel_list_1=[]\n",
    "\tfor i in range(5):\n",
    "\t\tmodel=model_1_1(train_1.shape[1])\n",
    "\t\tmodel.load_weights('Saved_Models/No_Events/Neural_Network_1/Model_1_1_'+str(i+1)+'.h5')\n",
    "\t\tmodel_list_1.append(model)\n",
    "\ttest_pred_avg_1_1=np.zeros((test_1.shape[0],12))\n",
    "\tfor i in range(len(model_list_1)):\n",
    "\t\ttest_pred=model_list_1[i].predict_proba(test_1)\n",
    "\t\ttest_pred_avg_1_1+=test_pred\n",
    "\ttest_pred_avg_1_1/=len(model_list_1)\n",
    "\t#Loading Saved Model_1_2 Neural Network Model\n",
    "\tmodel_1_2=create_model_1_2(train_1.shape[1],12)\n",
    "\tmodel_1_2.load_weights('Saved_Models/No_Events/Model_1_2.h5')\n",
    "\ttest_pred_1_2=model_1_2.predict_proba(test_1)\n",
    "\ty_data_events=events_train_data['Class'].values\n",
    "\ttrain_2, cv_2, y_train_2, y_cv_2 = train_test_split(X_train_events_one_hot_1, y_data_events,stratify=y_data_events,test_size=0.2,random_state=9)\n",
    "\ttest_2=X_test_events_one_hot_1\n",
    "\t#Loading All 20 Saved Model_2_1 Neural Network Models\n",
    "\tmodel_list_2=[]\n",
    "\tfor i in range(20):\n",
    "\t\tmodel=model_2_1(train_2.shape[1],12)\n",
    "\t\tmodel.load_weights('Saved_Models/Events/Neural_Network_1/Model_2_1_'+str(i+1)+'.h5')\n",
    "\t\tmodel_list_2.append(model)\n",
    "\ttest_pred_avg_2_1=np.zeros((test_2.shape[0],12))\n",
    "\tfor i in range(len(model_list_2)):\n",
    "\t\ttest_pred=model_list_2[i].predict_proba(test_2)\n",
    "\t\ttest_pred_avg_2_1+=test_pred\n",
    "\ttest_pred_avg_2_1/=len(model_list_2)\n",
    "\t#Loading All 20 Saved Model_2_2 Neural Network Models\n",
    "\tmodel_list_3=[]\n",
    "\tfor i in range(20):\n",
    "\t\tmodel=model_2_2(train_2.shape[1],12)\n",
    "\t\tmodel.load_weights('Saved_Models/Events/Neural_Network_2/Model_2_2_'+str(i+1)+'.h5')\n",
    "\t\tmodel_list_3.append(model)\n",
    "\ttest_pred_avg_2_2=np.zeros((test_2.shape[0],12))\n",
    "\tfor i in range(len(model_list_3)):\n",
    "\t\ttest_pred=model_list_3[i].predict_proba(test_2)\n",
    "\t\ttest_pred_avg_2_2+=test_pred\n",
    "\ttest_pred_avg_2_2/=len(model_list_3)\n",
    "\tprint(\"Models Predictions Done Time Taken:\",datetime.now()-start)\n",
    "\tprint(\"Ensembling Models......\")\n",
    "\tw1_1=0.15\n",
    "\tw1_2=0.75\n",
    "\tw1_3=0.1\n",
    "\tw2_1=0.5\n",
    "\tw2_2=0.5\n",
    "\t#Esembling and Calculating weighted average predictions\n",
    "\tTest_Prediction_1=(w1_1*lr_no_events_test_prediction)+(w1_2*test_pred_avg_1_1)+(w1_3*test_pred_1_2)\n",
    "\tTest_Prediction_2=(w2_1*test_pred_avg_2_1)+(w2_2*test_pred_avg_2_2)\n",
    "\tprint(\"Returned Test Predictions for Submission\")\n",
    "\tprint(\"Total Time Taken: \",datetime.now()-start)\n",
    "\tgatrain=pd.read_csv('gender_age_train.csv',index_col = 'device_id')\n",
    "\ttargetencoder = LabelEncoder().fit(gatrain.group)\n",
    "\ty = targetencoder.transform(gatrain.group)\n",
    "\tnclasses = len(targetencoder.classes_)\n",
    "\tpred_1 = pd.DataFrame(Test_Prediction_1, index = noevents_test_data.index, columns=targetencoder.classes_)\n",
    "\tpred_2 = pd.DataFrame(Test_Prediction_2, index = events_test_data.index, columns=targetencoder.classes_)\n",
    "\tfinal_pred=pd.concat([pred_1,pred_2], axis=0)\n",
    "\treturn final_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models Predictions Done Time Taken: 0:30:01.776757\n",
      "Ensembling Models......\n",
      "Returned Test Predictions for Submission\n",
      "Total Time Taken:  0:30:01.920034\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F23-</th>\n",
       "      <th>F24-26</th>\n",
       "      <th>F27-28</th>\n",
       "      <th>F29-32</th>\n",
       "      <th>F33-42</th>\n",
       "      <th>F43+</th>\n",
       "      <th>M22-</th>\n",
       "      <th>M23-26</th>\n",
       "      <th>M27-28</th>\n",
       "      <th>M29-31</th>\n",
       "      <th>M32-38</th>\n",
       "      <th>M39+</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>device_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-5893464122623104785</th>\n",
       "      <td>0.041951</td>\n",
       "      <td>0.057156</td>\n",
       "      <td>0.043716</td>\n",
       "      <td>0.067235</td>\n",
       "      <td>0.062994</td>\n",
       "      <td>0.046982</td>\n",
       "      <td>0.080628</td>\n",
       "      <td>0.153986</td>\n",
       "      <td>0.092507</td>\n",
       "      <td>0.109296</td>\n",
       "      <td>0.141558</td>\n",
       "      <td>0.101991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-7560708697029818408</th>\n",
       "      <td>0.041951</td>\n",
       "      <td>0.057156</td>\n",
       "      <td>0.043716</td>\n",
       "      <td>0.067235</td>\n",
       "      <td>0.062994</td>\n",
       "      <td>0.046982</td>\n",
       "      <td>0.080628</td>\n",
       "      <td>0.153986</td>\n",
       "      <td>0.092507</td>\n",
       "      <td>0.109296</td>\n",
       "      <td>0.141558</td>\n",
       "      <td>0.101991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289797889702373958</th>\n",
       "      <td>0.057709</td>\n",
       "      <td>0.058022</td>\n",
       "      <td>0.046506</td>\n",
       "      <td>0.068385</td>\n",
       "      <td>0.082696</td>\n",
       "      <td>0.064892</td>\n",
       "      <td>0.090617</td>\n",
       "      <td>0.115479</td>\n",
       "      <td>0.071335</td>\n",
       "      <td>0.100095</td>\n",
       "      <td>0.128950</td>\n",
       "      <td>0.115312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-402874006399730161</th>\n",
       "      <td>0.057709</td>\n",
       "      <td>0.058022</td>\n",
       "      <td>0.046506</td>\n",
       "      <td>0.068385</td>\n",
       "      <td>0.082696</td>\n",
       "      <td>0.064892</td>\n",
       "      <td>0.090617</td>\n",
       "      <td>0.115479</td>\n",
       "      <td>0.071335</td>\n",
       "      <td>0.100095</td>\n",
       "      <td>0.128950</td>\n",
       "      <td>0.115312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5751283639860028129</th>\n",
       "      <td>0.052995</td>\n",
       "      <td>0.064885</td>\n",
       "      <td>0.045246</td>\n",
       "      <td>0.068813</td>\n",
       "      <td>0.084061</td>\n",
       "      <td>0.067575</td>\n",
       "      <td>0.073340</td>\n",
       "      <td>0.124023</td>\n",
       "      <td>0.074047</td>\n",
       "      <td>0.099251</td>\n",
       "      <td>0.140322</td>\n",
       "      <td>0.105441</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          F23-    F24-26    F27-28    F29-32    F33-42  \\\n",
       "device_id                                                                \n",
       "-5893464122623104785  0.041951  0.057156  0.043716  0.067235  0.062994   \n",
       "-7560708697029818408  0.041951  0.057156  0.043716  0.067235  0.062994   \n",
       " 289797889702373958   0.057709  0.058022  0.046506  0.068385  0.082696   \n",
       "-402874006399730161   0.057709  0.058022  0.046506  0.068385  0.082696   \n",
       " 5751283639860028129  0.052995  0.064885  0.045246  0.068813  0.084061   \n",
       "\n",
       "                          F43+      M22-    M23-26    M27-28    M29-31  \\\n",
       "device_id                                                                \n",
       "-5893464122623104785  0.046982  0.080628  0.153986  0.092507  0.109296   \n",
       "-7560708697029818408  0.046982  0.080628  0.153986  0.092507  0.109296   \n",
       " 289797889702373958   0.064892  0.090617  0.115479  0.071335  0.100095   \n",
       "-402874006399730161   0.064892  0.090617  0.115479  0.071335  0.100095   \n",
       " 5751283639860028129  0.067575  0.073340  0.124023  0.074047  0.099251   \n",
       "\n",
       "                        M32-38      M39+  \n",
       "device_id                                 \n",
       "-5893464122623104785  0.141558  0.101991  \n",
       "-7560708697029818408  0.141558  0.101991  \n",
       " 289797889702373958   0.128950  0.115312  \n",
       "-402874006399730161   0.128950  0.115312  \n",
       " 5751283639860028129  0.140322  0.105441  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_df=final_fun_1('gender_age_test.csv')\n",
    "predicted_df.head()\n",
    "predicted_df.to_csv('Submission_Final_1.csv',index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_fun_2(X,Y):\n",
    "\t\"\"\"This function will take test input and target variable then returns the multi class log-loss\"\"\"\n",
    "\timport pandas as pd\n",
    "\timport numpy as np\n",
    "\timport matplotlib.pyplot as plt\n",
    "\t%matplotlib inline\n",
    "\timport seaborn as sns\n",
    "\timport os\n",
    "\timport re\n",
    "\timport time\n",
    "\tfrom sklearn.metrics import confusion_matrix\n",
    "\tfrom sklearn.metrics.classification import log_loss\n",
    "\tfrom sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\tfrom sklearn.feature_extraction.text import CountVectorizer\n",
    "\tfrom collections import Counter\n",
    "\tfrom scipy.sparse import hstack\n",
    "\tfrom scipy.sparse import csr_matrix\n",
    "\timport warnings\n",
    "\twarnings.filterwarnings(\"ignore\")\n",
    "\tfrom datetime import datetime\n",
    "\tfrom sklearn.model_selection import train_test_split\n",
    "\tfrom sklearn.linear_model import LogisticRegression\n",
    "\tfrom sklearn.ensemble import RandomForestClassifier\n",
    "\tfrom sklearn.calibration import CalibratedClassifierCV\n",
    "\tfrom xgboost import XGBClassifier\n",
    "\tfrom sklearn.model_selection import RandomizedSearchCV\n",
    "\tfrom sklearn.preprocessing import LabelEncoder\n",
    "\tfrom sklearn.preprocessing import StandardScaler\n",
    "\tfrom keras.models import Sequential\n",
    "\tfrom keras.layers import Dense, Dropout, Activation, BatchNormalization,Input,PReLU\n",
    "\tfrom keras.utils import np_utils\n",
    "\tfrom keras.optimizers import Adam\n",
    "\tfrom keras.models import Model\n",
    "\tfrom keras.optimizers import Adagrad\n",
    "\tfrom sklearn.externals import joblib\n",
    "\tstart=datetime.now()\n",
    "\ttrain_data=pd.read_csv('Train_Data_with_events_details.csv')\n",
    "\ttest_data=pd.read_csv('Test_Data_with_evetns_details.csv')\n",
    "\tgatest1=pd.read_csv(Y)\n",
    "\ttrain_data.drop(['Unnamed: 0'],axis=1,inplace=True)#removing the first column which does nt have any column name...\n",
    "\ttest_data.drop(['Unnamed: 0'],axis=1,inplace=True)#removing the first column which does not have any column name....\n",
    "\tevents_train_data=train_data.loc[train_data['has_events']==True]#To get data with events\n",
    "\tevents_test_data=test_data.loc[test_data['has_events']==True]\n",
    "\tnoevents_train_data=train_data.loc[train_data['has_events']==False]#to get data without events\n",
    "\tnoevents_test_data=test_data.loc[test_data['has_events']==False]\n",
    "\tevents_train_data.drop(['has_events'],axis=1,inplace=True)# we are keeping only device id by dropping has_events column\n",
    "\tevents_test_data.drop(['has_events'],axis=1,inplace=True)\n",
    "\tevents = pd.read_csv('events.csv',  parse_dates=['timestamp'], index_col='event_id')\n",
    "\tphone_data=pd.read_csv('phone_brand_device_model.csv')\n",
    "\t# we are droping Duplicate Devices and set Device_id as index like we did for Data while importing\n",
    "\tphone_data = phone_data.drop_duplicates('device_id',keep='first').set_index('device_id')\n",
    "\t# we take event_id, app_id and is_active.we dont take is_installed feature as it is not much useful\n",
    "\tapp_events = pd.read_csv('app_events.csv', usecols=['event_id','app_id','is_active'], dtype={'is_active':bool})#we are treating is_active data type as boolean\n",
    "\tapp_labels = pd.read_csv('app_labels.csv')\n",
    "\tlabel_categories = pd.read_csv('label_categories.csv')\n",
    "\ttrain_data=train_data.set_index('device_id')\n",
    "\ttest_data=test_data.set_index('device_id')\n",
    "\tevents_train_data=events_train_data.set_index('device_id')\n",
    "\tevents_test_data=events_test_data.set_index('device_id')\n",
    "\tnoevents_train_data=noevents_train_data.set_index('device_id')\n",
    "\tnoevents_test_data=noevents_test_data.set_index('device_id')\n",
    "\t#we will use numpy.arange function to create row numbers \n",
    "\ttrain_data['trainrow']=np.arange(train_data.shape[0])#all devices\n",
    "\tevents_train_data['trainrow']=np.arange(events_train_data.shape[0])#devices with events\n",
    "\ttest_data['testrow']=np.arange(test_data.shape[0])#all devices\n",
    "\tevents_test_data['testrow']=np.arange(events_test_data.shape[0])#devices with events\n",
    "\tnoevents_train_data['trainrow']=np.arange(noevents_train_data.shape[0])#devices without events\n",
    "\tnoevents_test_data['testrow']=np.arange(noevents_test_data.shape[0])#devices without events\n",
    "\t#Converting Categorical Brands in phone data to Integers from 0 to number of unique brands -1\n",
    "\tbrand_encoder = LabelEncoder().fit(phone_data['phone_brand'])\n",
    "\tphone_data['brand'] = brand_encoder.transform(phone_data['phone_brand'])\n",
    "\tnbrands=len(brand_encoder.classes_)# number of unique apps it will be used in creating One-Hot Encoding of Brands\n",
    "\t#Concatinating Phone Brand and Model \n",
    "\tconcat_model = phone_data['phone_brand'].str.cat(phone_data['device_model'])\n",
    "\t#Converting Categorical Maodel in phone data  to Integers from 0 to number of unique models -1\n",
    "\tmodel_encoder=LabelEncoder().fit(concat_model)\n",
    "\tphone_data['model_brand']=model_encoder.transform(concat_model)\n",
    "\tnmodels=len(model_encoder.classes_)# number of unique apps it will be used in creating One-Hot Encoding of Models\n",
    "\tmodel_encode=LabelEncoder().fit(phone_data['device_model'])\n",
    "\tphone_data['model']=model_encode.transform(phone_data['device_model'])\n",
    "\tnum_models=len(model_encoder.classes_)\n",
    "\t#Both the dataframes have device_id as indices so phone_brand and model corresponding to a device_id get copied to appropriate\n",
    "#columns\n",
    "\ttrain_data['phone_brand']=phone_data['brand']#adding encoded brand data to train data\n",
    "\ttest_data['phone_brand']=phone_data['brand']#adding encoded brand data to test data\n",
    "\ttrain_data['phone_model']=phone_data['model']#adding encoded model data to train data\n",
    "\ttest_data['phone_model']=phone_data['model']#adding encoded model data to test data\n",
    "\tevents_train_data['phone_brand']=phone_data['brand']#adding encoded brand data to train data with events\n",
    "\tevents_test_data['phone_brand']=phone_data['brand']#adding encoded brand data to test data with events\n",
    "\tevents_train_data['phone_model']=phone_data['model_brand']#addding encoded model+brand data to train data with events\n",
    "\tevents_test_data['phone_model']=phone_data['model_brand']#adding encoded model+brand data to test data with events\n",
    "\t#we dont add encoded model+brand data to nonevents data\n",
    "\tnoevents_train_data['phone_brand']=phone_data['brand']#adding encoded brand data to train data without events\n",
    "\tnoevents_test_data['phone_brand']=phone_data['brand']#adding encoded brand data to test data without events\n",
    "\tnoevents_train_data['phone_model']=phone_data['model']#adding encoded model data to train data without events\n",
    "\tnoevents_test_data['phone_model']=phone_data['model']#adding encoded model data to test data without events \n",
    "\tapp_encoder = LabelEncoder().fit(app_events['app_id'])\n",
    "\tapp_events['app'] = app_encoder.transform(app_events['app_id'])\n",
    "\tnapps = len(app_encoder.classes_)# number of unique apps it will be used in creating One-Hot Encoding of Apps \n",
    "\tdeviceapps = (app_events.merge(events[['device_id']], how='left',left_on='event_id',right_index=True)\n",
    "                       .groupby(['device_id','app'])['app'].agg(['size'])\n",
    "                       .merge(events_train_data[['trainrow']], how='left', left_index=True, right_index=True)#merges by index which is device_id from train data with events\n",
    "                       .merge(events_test_data[['testrow']], how='left', left_index=True, right_index=True)#merges by index which is device_id from test data with events\n",
    "                       .reset_index())\n",
    "\tapp_labels = app_labels.loc[app_labels['app_id'].isin(app_events['app_id'].unique())]#we are taking app_id's existed both in app_events and app_labels\n",
    "\tapp_labels['app'] = app_encoder.transform(app_labels['app_id'])#encoding common app_id's from above line\n",
    "\tlabelencoder = LabelEncoder().fit(app_labels['label_id'])#now encoding app_labels\n",
    "\tapp_labels['label'] = labelencoder.transform(app_labels['label_id'])\n",
    "\tnlabels = len(labelencoder.classes_)\n",
    "\tdevicelabels = (deviceapps[['device_id','app']]\n",
    "                .merge(app_labels[['app','label']])\n",
    "                .groupby(['device_id','label'])['app'].agg(['size'])\n",
    "                .merge(events_train_data[['trainrow']], how='left', left_index=True, right_index=True)#merges by index which is device_id from train data with events\n",
    "                .merge(events_test_data[['testrow']], how='left', left_index=True, right_index=True)#merges by index which is device_id from test data with events\n",
    "                .reset_index())#resetting index to default indices\n",
    "\t#Extracting Hours from Events Time Stamp by using map function \n",
    "\tevents['hour'] = events['timestamp'].map(lambda x:pd.to_datetime(x).hour)\n",
    "\tevents['hourbin'] = [1 if ((x>=1)&(x<=6)) else 2 if ((x>=7)&(x<=12)) else 3 if ((x>=13)&(x<=18)) else 4 for x in events['hour']]\n",
    "\thourevents = events.groupby(\"device_id\")[\"hour\"].apply(lambda x: \" \".join('0'+str(s) for s in x))\n",
    "\thourbinevents = events.groupby(\"device_id\")[\"hourbin\"].apply(lambda x: \" \".join('0'+str(s) for s in x))\n",
    "\t#Mapping The Values the values to train and test dataframes\n",
    "\tevents_train_data['event_hours']=events_train_data.index.map(hourevents)#mapping hourevents to train data with events based on index deviceid\n",
    "\tevents_test_data['event_hours']=events_test_data.index.map(hourevents)#mapping hourevents to test data with events based on index deviceid\n",
    "\tevents_train_data['event_hours_bins']=events_train_data.index.map(hourbinevents)#mapping houreventsbin to train data with events based on index deviceid\n",
    "\tevents_test_data['event_hours_bins']=events_test_data.index.map(hourbinevents)#mapping houreventsbins to test data with events based on index deviceid\n",
    "\t#Extracting Days of the week\n",
    "\tdays_of_week=events['timestamp'].dt.day_name()\n",
    "\tevents['day']=days_of_week.map({'Sunday':0,'Monday':1,'Tuesday':2,'Wednesday':3,'Thursday':4,'Friday':5,'Saturday':6})#mapping day to numeric digits\n",
    "\tdaysevents = events.groupby(\"device_id\")[\"day\"].apply(lambda x: \" \".join('0'+str(s) for s in x))#concatening '0' to all single nubmerals\n",
    "\tevents_train_data['event_day']=events_train_data.index.map(daysevents)#mapping days to train data with events\n",
    "\tevents_test_data['event_day']=events_test_data.index.map(daysevents)#mapping days to test data with events\n",
    "\t#Grouping by device_id and taking the median of latitude\n",
    "\tlat_events = events.groupby(\"device_id\")[\"latitude\"].apply(lambda x: np.median([float(s) for s in x]))\n",
    "\t#Grouping by device_id and taking the median of longitude\n",
    "\tlong_events = events.groupby(\"device_id\")[\"longitude\"].apply(lambda x: np.median([float(s) for s in x]))\n",
    "\tevents_train_data['event_med_lat']=events_train_data.index.map(lat_events)#mapping lattitude_medians to events_train_data\n",
    "\tevents_test_data['event_med_lat']=events_test_data.index.map(lat_events)#mapping lattitude_medians to events_test_data\n",
    "\tevents_train_data['event_med_long']=events_train_data.index.map(long_events)#mapping longitude_medians to events_train_data\n",
    "\tevents_test_data['event_med_long']=events_test_data.index.map(long_events)#mapping longitude_medians to events_test_data\n",
    "\tappsactive = app_events.groupby(\"event_id\")[\"is_active\"].apply(lambda x: \" \".join(str(s) for s in x))#converting boolean to strings\n",
    "\t#Mapping apps is_active to device_id\n",
    "\tevents[\"apps_active\"] = events.index.map(appsactive)#mapping converted strings to events\n",
    "\tevents_apps_active_map = events.groupby(\"device_id\")[\"apps_active\"].apply(lambda x: \" \".join(str(s) for s in x if str(s)!='nan'))#mapping only non-nan(status of is_active) acitve apps\n",
    "\t#Mapping The Values the values to train and test dataframes\n",
    "\tevents_train_data['apps_active']=events_train_data.index.map(events_apps_active_map)#mapping non-nan statused apps to train data with events\n",
    "\tevents_test_data['apps_active']=events_test_data.index.map(events_apps_active_map)#mapping non-nan statused apps to test data with events\n",
    "\ttrain_data_store=events_train_data.reset_index()\n",
    "\ttest_data_store=events_test_data.reset_index()\n",
    "\ttrain_data_noevents=noevents_train_data.reset_index()\n",
    "\ttest_data_noevents=noevents_test_data.reset_index()\n",
    "\t#Storing the Processed Events Train Data\n",
    "\ttrain_data_store.to_csv('Processed_events_train_data.csv')\n",
    "\t#Storing the Processed Events Test\n",
    "\ttest_data_store.to_csv('Processed_events_test_data.csv')\n",
    "\t#Storing the Processed No Events Train Data\n",
    "\ttrain_data_noevents.to_csv('Processed_no_events_train_data.csv')\n",
    "\t#Storing the Processed No Events Test\n",
    "\ttest_data_noevents.to_csv('Processed_no_events_test_data.csv')\n",
    "\t#https://stackoverflow.com/questions/8955448/save-load-scipy-sparse-csr-matrix-in-portable-data-format\n",
    "\tdef save_sparse(filename, xmtr):\n",
    "\t\t\"\"\"\n",
    "\t\tTakes the Sparse Matrix, file name as input and saves the Matrix as npz file\n",
    "\t\t\"\"\"\n",
    "\t\tnp.savez(filename,data = xmtr.data ,indices= xmtr.indices,\n",
    "             indptr =xmtr.indptr, shape=xmtr.shape )\n",
    "\t#https://stackoverflow.com/questions/8955448/save-load-scipy-sparse-csr-matrix-in-portable-data-format\n",
    "\tdef load_sparse(filename):\n",
    "\t\t\"\"\"\n",
    "\t\tTakes the filename as input and loads the matrix stored as .npz file to a csr matrix and returns the matrix\n",
    "\t\t\"\"\"\n",
    "\t\ttmp = np.load(filename)\n",
    "\t\treturn csr_matrix((tmp['data'], tmp['indices'], tmp['indptr']), shape= tmp['shape'])\n",
    "\tXtr_brand = csr_matrix((np.ones(train_data.shape[0]), # Number of Rows/Devices\n",
    "                       (train_data.trainrow, train_data.phone_brand)))# columns as brands \n",
    "\tXte_brand = csr_matrix((np.ones(test_data.shape[0]), # Number of Rows/Devices\n",
    "                       (test_data.testrow, test_data.phone_brand)))\n",
    "\tXtr_model = csr_matrix((np.ones(train_data.shape[0]), # Number of Rows/Devices\n",
    "                       (train_data.trainrow, train_data.phone_model)))\n",
    "\tXte_model = csr_matrix((np.ones(test_data.shape[0]), # Number of Rows/Devices\n",
    "                       (test_data.testrow, test_data.phone_model)))\n",
    "\tX_train_one_hot=hstack((Xtr_brand,Xtr_model),format='csr')\n",
    "\tX_test_one_hot=hstack((Xte_brand,Xte_model),format='csr')\n",
    "\tsave_sparse('Train_One_hot_brand_model_matrix',X_train_one_hot)\n",
    "\tsave_sparse('Test_One_hot_brand_model_matrix',X_test_one_hot)\n",
    "\t#https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html\n",
    "\tXtr_noevents_brand = csr_matrix((np.ones(noevents_train_data.shape[0]), # Number of Rows/Devices\n",
    "                       (noevents_train_data.trainrow, noevents_train_data.phone_brand)))\n",
    "\tXte_noevents_brand = csr_matrix((np.ones(noevents_test_data.shape[0]), # Number of Rows/Devices\n",
    "                       (noevents_test_data.testrow, noevents_test_data.phone_brand)))\n",
    "\tXtr_noevents_model = csr_matrix((np.ones(noevents_train_data.shape[0]), # Number of Rows/Devices\n",
    "                       (noevents_train_data.trainrow, noevents_train_data.phone_model)))\n",
    "\tXte_noevents_model = csr_matrix((np.ones(noevents_test_data.shape[0]), # Number of Rows/Devices\n",
    "                       (noevents_test_data.testrow, noevents_test_data.phone_model)))\n",
    "\tX_train_noevents_one_hot=hstack((Xtr_noevents_brand,Xtr_noevents_model),format='csr')\n",
    "\tX_test_noevents_one_hot=hstack((Xte_noevents_brand,Xte_noevents_model),format='csr')\n",
    "\t#Saving One-hot encoded Matrices\n",
    "\tsave_sparse('Train_Noevents_One_hot_brand_model_matrix',X_train_noevents_one_hot)\n",
    "\tsave_sparse('Test_Noevents_One_hot_brand_model_matrix',X_test_noevents_one_hot)\n",
    "\t#https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html\n",
    "\tXtr_events_brand = csr_matrix((np.ones(events_train_data.shape[0]), # Number of Rows/Devices\n",
    "                       (events_train_data.trainrow, events_train_data.phone_brand)), \n",
    "                              shape=(events_train_data.shape[0],nbrands))\n",
    "\tXte_events_brand = csr_matrix((np.ones(events_test_data.shape[0]), # Number of Rows/Devices\n",
    "                       (events_test_data.testrow, events_test_data.phone_brand)),\n",
    "                             shape=(events_test_data.shape[0],nbrands))\n",
    "\t#https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html\n",
    "\tXtr_events_model = csr_matrix((np.ones(events_train_data.shape[0]), # Number of Rows/Devices\n",
    "                       (events_train_data.trainrow, events_train_data.phone_model)),\n",
    "                         shape=(events_train_data.shape[0],nmodels))\n",
    "\tXte_events_model = csr_matrix((np.ones(events_test_data.shape[0]), # Number of Rows/Devices\n",
    "                       (events_test_data.testrow, events_test_data.phone_model)),\n",
    "                           shape=(events_test_data.shape[0],nmodels))\n",
    "\t#Since the Deviceapps has both train and test columns merged to create Train Apps One-Hot we will Drop all Nan of Train Row\n",
    "\t#Once we remove Nan in Train Rows we will get the Apps in Train Data and we create CSR Matrix for those rows\n",
    "\td = deviceapps.dropna(subset=['trainrow'])#based on trainrow we drop\n",
    "\tXtr_events_app = csr_matrix((np.ones(d.shape[0]), (d.trainrow, d.app)), \n",
    "                      shape=(events_train_data.shape[0],napps))\n",
    "\t#Since the Deviceapps has both train and test columns merged to create Test Apps One-Hot we will Drop all Nan of Test Row\n",
    "\t#Once we remove Nan in Test Rows we will get the Apps in Test Data and we create CSR Matrix for those rows\n",
    "\td = deviceapps.dropna(subset=['testrow'])#based on test row we drop\n",
    "\tXte_events_app = csr_matrix((np.ones(d.shape[0]), (d.testrow, d.app)), \n",
    "                      shape=(events_test_data.shape[0],napps))\n",
    "\t#Since the Devicelabels has both train and test columns merged to create Train Labels One-Hot we will Drop all Nan of Train Row\n",
    "\t#Once we remove Nan in Train Rows we will get the Labels in Train Data and we create CSR Matrix for those rows\n",
    "\td = devicelabels.dropna(subset=['trainrow'])##based on trainrow we drop\n",
    "\tXtr_events_label = csr_matrix((np.ones(d.shape[0]), (d.trainrow, d.label)), \n",
    "                      shape=(events_train_data.shape[0],nlabels))\n",
    "\t#Since the Devicelabels has both train and test columns merged to create Test Labels One-Hot we will Drop all Nan of Test Row\n",
    "\t#Once we remove Nan in Test Rows we will get the Labels in Test Data and we create CSR Matrix for those rows\n",
    "\td = devicelabels.dropna(subset=['testrow'])#based on test row  we drop\n",
    "\tXte_events_label = csr_matrix((np.ones(d.shape[0]), (d.testrow, d.label)), \n",
    "                      shape=(events_test_data.shape[0],nlabels))\n",
    "\tvectorizer_EH=TfidfVectorizer()\n",
    "\tvectorizer_EH.fit(events_train_data['event_hours'].values)\n",
    "\n",
    "\tX_tr_event_hours_one_hot = vectorizer_EH.transform(events_train_data['event_hours'].values)\n",
    "\tX_te_event_hours_one_hot = vectorizer_EH.transform(events_test_data['event_hours'].values)\n",
    "\tvectorizer_EHB=CountVectorizer(binary=True)\n",
    "\tvectorizer_EHB.fit(events_train_data['event_hours_bins'].values)\n",
    "\tX_tr_event_hours_bins_one_hot = vectorizer_EHB.transform(events_train_data['event_hours_bins'].values)\n",
    "\tX_te_event_hours_bins_one_hot = vectorizer_EHB.transform(events_test_data['event_hours_bins'].values)\n",
    "\tvectorizer_ED=TfidfVectorizer()\n",
    "\tvectorizer_ED.fit(events_train_data['event_day'].values)\n",
    "\tX_tr_event_day_one_hot = vectorizer_ED.transform(events_train_data['event_day'].values)\n",
    "\tX_te_event_day_one_hot = vectorizer_ED.transform(events_test_data['event_day'].values)\n",
    "\tscaler_lat=StandardScaler()\n",
    "\tscaler_lat.fit(events_train_data['event_med_lat'].values.reshape(-1,1))\n",
    "\n",
    "\tX_tr_event_med_lat_scaled = scaler_lat.transform(events_train_data['event_med_lat'].values.reshape(-1,1))\n",
    "\tX_te_event_med_lat_scaled = scaler_lat.transform(events_test_data['event_med_lat'].values.reshape(-1,1))\n",
    "\tscaler_long=StandardScaler()\n",
    "\tscaler_long.fit(events_train_data['event_med_long'].values.reshape(-1,1))\n",
    "\n",
    "\tX_tr_event_med_long_scaled = scaler_long.transform(events_train_data['event_med_long'].values.reshape(-1,1))\n",
    "\tX_te_event_med_long_scaled = scaler_long.transform(events_test_data['event_med_long'].values.reshape(-1,1))\n",
    "\tvectorizer_AA=TfidfVectorizer()\n",
    "\tvectorizer_AA.fit(events_train_data['apps_active'].values)\n",
    "\n",
    "\tX_tr_apps_active_one_hot = vectorizer_AA.transform(events_train_data['apps_active'].values)\n",
    "\tX_te_apps_active_one_hot = vectorizer_AA.transform(events_test_data['apps_active'].values)\n",
    "\t#Converting to Sparse Matrices\n",
    "\tX_tr_event_hours_one_hot=X_tr_event_hours_one_hot.tocsr()\n",
    "\tX_te_event_hours_one_hot=X_te_event_hours_one_hot.tocsr()\n",
    "\n",
    "\tX_tr_event_hours_bins_one_hot=X_tr_event_hours_bins_one_hot.tocsr()\n",
    "\tX_te_event_hours_bins_one_hot=X_te_event_hours_bins_one_hot.tocsr()\n",
    "\n",
    "\tX_tr_event_day_one_hot=X_tr_event_day_one_hot.tocsr()\n",
    "\tX_te_event_day_one_hot=X_te_event_day_one_hot.tocsr()\n",
    "\n",
    "\tX_tr_apps_active_one_hot=X_tr_apps_active_one_hot.tocsr()\n",
    "\tX_te_apps_active_one_hot=X_te_apps_active_one_hot.tocsr()\n",
    "\n",
    "\tX_train_events_one_hot_1=hstack((Xtr_events_brand,Xtr_events_model,Xtr_events_label,X_tr_event_hours_one_hot,X_tr_event_hours_bins_one_hot,X_tr_event_day_one_hot,X_tr_event_med_lat_scaled,X_tr_event_med_long_scaled,Xtr_events_app,X_tr_apps_active_one_hot),format='csr')\n",
    "\tX_test_events_one_hot_1=hstack((Xte_events_brand,Xte_events_model,Xte_events_label,X_te_event_hours_one_hot,X_te_event_hours_bins_one_hot,X_te_event_day_one_hot,X_te_event_med_lat_scaled,X_te_event_med_long_scaled,Xte_events_app,X_te_apps_active_one_hot),format='csr')\n",
    "\t#Saving One-hot encoded Matrices\n",
    "\tsave_sparse('Train_Events_One_hot_matrix_1',X_train_events_one_hot_1)\n",
    "\tsave_sparse('Test_Events_One_hot_matrix_1',X_test_events_one_hot_1)\n",
    "\ty_data=train_data['Class'].values\n",
    "\ttrain_1, cv_1, y_train_1, y_cv_1 = train_test_split(X_train_one_hot, y_data,stratify=y_data,test_size=0.15,random_state=18)\n",
    "\ttest_1=X_test_noevents_one_hot#because we want to test on no events data\n",
    "\tlr_sig_clf=joblib.load('calibrated_LR_no_events.sav')\n",
    "\tlr_no_events_test_prediction=lr_sig_clf.predict_proba(test_1)\n",
    "\t#Loading All 5 Saved Model_1_1 Neural Network  Models\n",
    "\tdef model_1_1(input_shape):\n",
    "\t\tmodel = Sequential()\n",
    "\t\tmodel.add(Dense(256, input_dim=input_shape))\n",
    "\t\tmodel.add(PReLU())\n",
    "\t\tmodel.add(BatchNormalization())\n",
    "\t\tmodel.add(Dropout(0.5))\n",
    "\t\tmodel.add(Dense(64))\n",
    "\t\tmodel.add(PReLU())\n",
    "\t\tmodel.add(BatchNormalization())\n",
    "\t\tmodel.add(Dropout(0.5))\n",
    "\t\tmodel.add(Dense(12))\n",
    "\t\tmodel.add(Activation('softmax'))\n",
    "\t\tmodel.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\t\treturn model\n",
    "\tdef create_model_1_2(input_dim,output_dim, learRate=0.0025):\n",
    "\t\tmodel = Sequential()\n",
    "\t\tmodel.add(Dense(500, input_shape=(input_dim,), init='uniform'))\n",
    "\t\tmodel.add(PReLU(init='zero'))\n",
    "\t\tmodel.add(Dropout(0.82))\n",
    "\t\tmodel.add(Dense(output_dim, init='uniform'))\n",
    "\t\tmodel.add(Activation('softmax'))\n",
    "\t\topt = Adagrad(lr=learRate, epsilon=1e-08)\n",
    "\t\tmodel.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "\t\treturn model\n",
    "\tdef model_2_1(input_dim,output_dim):\n",
    "\t\tmodel = Sequential()\n",
    "\t\tmodel.add(Dropout(0.15, input_shape=(input_dim,)))\n",
    "\t\tmodel.add(Dense(240, init='uniform'))\n",
    "\t\tmodel.add(PReLU(init='zero'))\n",
    "\t\tmodel.add(Dropout(0.8))\n",
    "\t\tmodel.add(Dense(240, init='uniform'))\n",
    "\t\tmodel.add(PReLU(init='zero', weights=None))\n",
    "\t\tmodel.add(Dropout(0.35))\n",
    "\t\tmodel.add(Dense(260, init='uniform'))\n",
    "\t\tmodel.add(PReLU(init='zero', weights=None))\n",
    "\t\tmodel.add(Dropout(0.40))\n",
    "\t\tmodel.add(Dense(output_dim, init='uniform'))\n",
    "\t\tmodel.add(Activation('softmax'))\n",
    "\t\topt = Adagrad(lr=0.008, epsilon=1e-08)\n",
    "\t\tmodel.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "\t\treturn model\n",
    "\tdef model_2_2(input_dim,output_dim):\n",
    "\t\tmodel = Sequential()\n",
    "\t\tmodel.add(Dropout(0.4, input_shape=(input_dim,)))\n",
    "\t\tmodel.add(Dense(75))\n",
    "\t\tmodel.add(PReLU())\n",
    "\t\tmodel.add(Dropout(0.30))\n",
    "\t\tmodel.add(Dense(50, init='normal', activation='tanh'))\n",
    "\t\tmodel.add(PReLU())\n",
    "\t\tmodel.add(Dropout(0.20))\n",
    "\t\tmodel.add(Dense(output_dim, init='normal', activation='softmax'))\n",
    "\t\tmodel.compile(loss='categorical_crossentropy', optimizer='adadelta', metrics=['accuracy'])\n",
    "\t\treturn model\n",
    "\tmodel_list_1=[]\n",
    "\tfor i in range(5):\n",
    "\t\tmodel=model_1_1(train_1.shape[1])\n",
    "\t\tmodel.load_weights('Saved_Models/No_Events/Neural_Network_1/Model_1_1_'+str(i+1)+'.h5')\n",
    "\t\tmodel_list_1.append(model)\n",
    "\ttest_pred_avg_1_1=np.zeros((test_1.shape[0],12))\n",
    "\tfor i in range(len(model_list_1)):\n",
    "\t\ttest_pred=model_list_1[i].predict_proba(test_1)\n",
    "\t\ttest_pred_avg_1_1+=test_pred\n",
    "\ttest_pred_avg_1_1/=len(model_list_1)\n",
    "\t#Loading Saved Model_1_2 Neural Network Model\n",
    "\tmodel_1_2=create_model_1_2(train_1.shape[1],12)\n",
    "\tmodel_1_2.load_weights('Saved_Models/No_Events/Model_1_2.h5')\n",
    "\ttest_pred_1_2=model_1_2.predict_proba(test_1)\n",
    "\ty_data_events=events_train_data['Class'].values\n",
    "\ttrain_2, cv_2, y_train_2, y_cv_2 = train_test_split(X_train_events_one_hot_1, y_data_events,stratify=y_data_events,test_size=0.2,random_state=9)\n",
    "\ttest_2=X_test_events_one_hot_1\n",
    "\t#Loading All 20 Saved Model_2_1 Neural Network Models\n",
    "\tmodel_list_2=[]\n",
    "\tfor i in range(20):\n",
    "\t\tmodel=model_2_1(train_2.shape[1],12)\n",
    "\t\tmodel.load_weights('Saved_Models/Events/Neural_Network_1/Model_2_1_'+str(i+1)+'.h5')\n",
    "\t\tmodel_list_2.append(model)\n",
    "\ttest_pred_avg_2_1=np.zeros((test_2.shape[0],12))\n",
    "\tfor i in range(len(model_list_2)):\n",
    "\t\ttest_pred=model_list_2[i].predict_proba(test_2)\n",
    "\t\ttest_pred_avg_2_1+=test_pred\n",
    "\ttest_pred_avg_2_1/=len(model_list_2)\n",
    "\t#Loading All 20 Saved Model_2_2 Neural Network Models\n",
    "\tmodel_list_3=[]\n",
    "\tfor i in range(20):\n",
    "\t\tmodel=model_2_2(train_2.shape[1],12)\n",
    "\t\tmodel.load_weights('Saved_Models/Events/Neural_Network_2/Model_2_2_'+str(i+1)+'.h5')\n",
    "\t\tmodel_list_3.append(model)\n",
    "\ttest_pred_avg_2_2=np.zeros((test_2.shape[0],12))\n",
    "\tfor i in range(len(model_list_3)):\n",
    "\t\ttest_pred=model_list_3[i].predict_proba(test_2)\n",
    "\t\ttest_pred_avg_2_2+=test_pred\n",
    "\ttest_pred_avg_2_2/=len(model_list_3)\n",
    "\tprint(\"Models Predictions Done Time Taken:\",datetime.now()-start)\n",
    "\tprint(\"Ensembling Models......\")\n",
    "\tw1_1=0.15\n",
    "\tw1_2=0.75\n",
    "\tw1_3=0.1\n",
    "\tw2_1=0.5\n",
    "\tw2_2=0.5\n",
    "\t#Esembling and Calculating weighted average predictions\n",
    "\tTest_Prediction_1=(w1_1*lr_no_events_test_prediction)+(w1_2*test_pred_avg_1_1)+(w1_3*test_pred_1_2)\n",
    "\tTest_Prediction_2=(w2_1*test_pred_avg_2_1)+(w2_2*test_pred_avg_2_2)\n",
    "\tprint(\"Returned Test Predictions for Submission\")\n",
    "\tprint(\"Total Time Taken: \",datetime.now()-start)\n",
    "\tgatrain=pd.read_csv('gender_age_train.csv',index_col = 'device_id')\n",
    "\ttargetencoder = LabelEncoder().fit(gatrain.group)\n",
    "\ty = targetencoder.transform(gatrain.group)\n",
    "\tnclasses = len(targetencoder.classes_)\n",
    "\tpred_1 = pd.DataFrame(Test_Prediction_1, index = noevents_test_data.index, columns=targetencoder.classes_)\n",
    "\tpred_2 = pd.DataFrame(Test_Prediction_2, index = events_test_data.index, columns=targetencoder.classes_)\n",
    "\tfinal_pred=pd.concat([pred_1,pred_2], axis=0)\n",
    "\ttargetencoder1 = LabelEncoder().fit(gatest1.group)\n",
    "\ty_actual= targetencoder1.transform(gatest1.group)\n",
    "\treturn log_loss(y_actual,final_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models Predictions Done Time Taken: 0:27:02.235947\n",
      "Ensembling Models......\n",
      "Returned Test Predictions for Submission\n",
      "Total Time Taken:  0:27:02.360937\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.6680995509727947"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_fun_2('gender_age_test.csv','test_age_group.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
